# Model Card

## Overview

This model card provides information about various GPT models and their configurations. Each model is represented by a dictionary containing its configuration parameters.

## Model Configurations

### openai-gpt
- Number of layers: 12
- Number of attention heads: 12
- Embedding size: 768
- Params Counts: 117M

### gpt2
- Number of layers: 12
- Number of attention heads: 12
- Embedding size: 768
- Params Counts: 124M

### gpt2-medium
- Number of layers: 24
- Number of attention heads: 16
- Embedding size: 1024
- Params Counts: 350M

### gpt2-large
- Number of layers: 36
- Number of attention heads: 20
- Embedding size: 1280
- Params Counts: 774M

### gpt2-xl
- Number of layers: 48
- Number of attention heads: 25
- Embedding size: 1600
- Params Counts: 1558M

### gopher-44m
- Number of layers: 8
- Number of attention heads: 16
- Embedding size: 512

### gpt-mini
- Number of layers: 6
- Number of attention heads: 6
- Embedding size: 192

### gpt-micro
- Number of layers: 4
- Number of attention heads: 4
- Embedding size: 128

### gpt-nano
- Number of layers: 3
- Number of attention heads: 3
- Embedding size: 48
